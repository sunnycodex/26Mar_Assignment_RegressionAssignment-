{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b14276-071b-43ed-bd40-f9a8f554a177",
   "metadata": {},
   "source": [
    "#1\n",
    "\n",
    "Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b9395-a6af-4f3f-9bf5-8dcc992efebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "Simple Linear Regression (SLR): It involves one independent variable and one dependent variable. The relationship is represented by a straight line, hence the term ‘linear regression’.\n",
    "\n",
    "Example: Predicting a person’s weight based on their height. Here, weight is the dependent variable (what you want to predict), and height is the independent variable (the predictor).\n",
    "\n",
    "Multiple Linear Regression (MLR): It involves more than one independent variable and one dependent variable. This allows for more complex relationships to be modeled.\n",
    "\n",
    "Example: Predicting a person’s weight based on their height, age, and diet. Here, weight is still the dependent variable, but we now have three independent variables (height, age, and diet)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345570e0-b118-4ced-ab1d-f4ab6e664415",
   "metadata": {},
   "source": [
    "#2\n",
    "\n",
    "Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d61baa-534a-41f5-9a27-1fbbe80c58ba",
   "metadata": {},
   "source": [
    "---\n",
    "The assumptions of linear regression are:\n",
    "\n",
    "* **Linearity:** The relationship between the independent variable(s) and the dependent variable is linear. This means that the best fit line should be a straight line. You can check for linearity by plotting the independent variable against the dependent variable and looking for a straight line pattern.\n",
    "* **Homoscedasticity:** The variance of the residuals is constant across all values of the independent variable. This means that the vertical distances from the data points to the best fit line should be roughly the same for all values of the independent variable. You can check for homoscedasticity by plotting the residuals against the predicted values of the dependent variable. If the residuals are randomly scattered around the horizontal line, then the assumption of homoscedasticity is met.\n",
    "* **Independence:** The residuals are independent of each other. This means that the value of one residual does not affect the value of another residual. You can check for independence by plotting the residuals against the order of the data points. If there is no pattern to the residuals, then the assumption of independence is met.\n",
    "* **Normality:** The residuals are normally distributed. This means that the residuals should be bell-shaped and symmetrical around the mean. You can check for normality by plotting the residuals in a histogram. If the residuals are bell-shaped and symmetrical, then the assumption of normality is met.\n",
    "* **No multicollinearity:** The independent variables are not highly correlated with each other. This means that no independent variable can be perfectly predicted from the other independent variables. You can check for multicollinearity by calculating the correlation coefficients between the independent variables. If any of the correlation coefficients are close to 1, then there is multicollinearity.\n",
    "\n",
    "If any of these assumptions are not met, then the results of the linear regression analysis may not be reliable.\n",
    "\n",
    "Here are some specific statistical tests that can be used to check each of the assumptions of linear regression:\n",
    "\n",
    "* **Linearity:** You can use a scatter plot to check for linearity. If the data points form a straight line pattern, then the assumption of linearity is met.\n",
    "* **Homoscedasticity:** You can use the **Levene's test** to check for homoscedasticity. This test compares the variances of the residuals at different values of the independent variable. If the p-value of the Levene's test is non-significant, then the assumption of homoscedasticity is met.\n",
    "* **Independence:** You can use the **Durbin-Watson test** to check for independence. This test measures the autocorrelation of the residuals. If the Durbin-Watson statistic is between 2 and 3, then the assumption of independence is met.\n",
    "* **Normality:** You can use the **Shapiro-Wilk test** to check for normality. This test compares the distribution of the residuals to a normal distribution. If the p-value of the Shapiro-Wilk test is non-significant, then the assumption of normality is met.\n",
    "* **No multicollinearity:** You can use the **Variance Inflation Factor (VIF)** to check for multicollinearity. The VIF is a measure of how much one independent variable can be predicted from the other independent variables. If the VIF for any independent variable is greater than 10, then there is multicollinearity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106cd43c-f0f8-48e1-b635-162505e841ea",
   "metadata": {},
   "source": [
    "#3\n",
    "\n",
    "How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d50ca0-e0ac-4797-aa83-042724e1523b",
   "metadata": {},
   "source": [
    "---\n",
    "In a linear regression model, the slope and the intercept are parameters that determine the characteristics of the best-fit line.\n",
    "\n",
    "Slope (β1): The slope represents the average change in the dependent variable for a one unit increase in an independent variable, assuming all other variables are held constant. In other words, it’s the rate of change.\n",
    "\n",
    "Intercept (β0): The intercept represents the mean value of the dependent variable when all of the predictor variables in the model are equal to zero. It’s where the line crosses the y-axis.\n",
    "\n",
    "Here are some other real-world scenarios where linear regression can be used:\n",
    "\n",
    "Predicting the price of a house based on its square footage\n",
    "Predicting the number of sales a company makes based on its advertising spending\n",
    "Predicting the weight of a baby based on its gestational age\n",
    "Predicting the risk of heart disease based on a person's cholesterol level\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6388574b-284b-4ee2-835d-108dc57cabc8",
   "metadata": {},
   "source": [
    "#4\n",
    "\n",
    "Explain the concept of gradient descent. How is it used in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d987e7f-b452-467c-adc5-d4aa705b8bb2",
   "metadata": {},
   "source": [
    "---\n",
    "Gradient descent is an optimization algorithm commonly used to train machine learning models and neural networks. It works by iteratively adjusting the model's parameters to minimize the cost function, which measures the difference between the model's predictions and the actual results.\n",
    "\n",
    "The algorithm starts with an initial set of parameters and calculates the cost function. It then computes the gradient of the cost function with respect to the parameters, which indicates the direction in which the cost function is increasing. The algorithm updates the parameters by taking a step in the opposite direction of the gradient, with the size of the step determined by a hyperparameter called the learning rate.\n",
    "\n",
    "This process is repeated until the cost function reaches a minimum value, indicating that the model's predictions are as close as possible to the actual results. Once optimized, machine learning models can be powerful tools for artificial intelligence and various computer science applications.\n",
    "\n",
    "Gradient descent can be applied to various machine learning algorithms, including linear regression, logistic regression, neural networks, and support vector machines. It provides a general framework for optimizing models by iteratively refining their parameters based on the cost function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ad1f0-51e2-437f-9e97-8052e9140e18",
   "metadata": {},
   "source": [
    "#5\n",
    "\n",
    "Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2212f98-6c5d-49e2-8d5c-56557c1e8226",
   "metadata": {},
   "source": [
    "---\n",
    "Multiple Linear Regression (MLR) is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. The goal of MLR is to model the linear relationship between the explanatory (independent) variables and response(dependent) variable.\n",
    "\n",
    "In Simple Linear Regression (SLR), a single independent variable is used to predict the value of a dependent variable. In Multiple Linear Regression, two or more independent variables are used to predict the value of a dependent variable. The difference between the two is the number of independent variables. In both cases, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c672dbb-2f43-4b0d-b83a-d7cb5f7b05da",
   "metadata": {},
   "source": [
    "#6\n",
    "\n",
    "Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95322bd-ca43-4773-a2e8-7ee4df83b432",
   "metadata": {},
   "source": [
    "---\n",
    "Multicollinearity in multiple linear regression refers to a situation where two or more independent variables are highly correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results.\n",
    "\n",
    "To detect multicollinearity, you can use a metric known as the variance inflation factor (VIF), which measures the strength of correlation between predictor variables in a model. VIF takes on a value between 1 and positive infinity.\n",
    "\n",
    "To address multicollinearity, some common solutions include:\n",
    "\n",
    "Remove one or more of the highly correlated variables.\n",
    "\n",
    "Linearly combine the predictor variables in some way, such as adding or subtracting them from one way.\n",
    "\n",
    "Perform an analysis that is designed to account for highly correlated variables such as principal component analysis or partial least squares (PLS) regression.\n",
    "\n",
    "Techniques such as regularization or feature selection can be applied to select a subset of independent variables that are not highly correlated with each other.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086b203-5276-4648-864f-ce113dc2d01c",
   "metadata": {},
   "source": [
    "#7\n",
    "\n",
    "Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dbde7c-863b-4d81-bf72-acbc9e1d14bc",
   "metadata": {},
   "source": [
    "---\n",
    "Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y|x).\n",
    "\n",
    "The main difference between linear regression and polynomial regression is that linear regression assumes a linear relationship between the dependent and independent variables, while polynomial regression can model more complex, nonlinear relationships. In linear regression, a straight line is used to represent the relationship between the variables, while in polynomial regression, a polynomial function is used to represent the relationship.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5237b3-f13e-4cca-8bd0-ee52a71d1ac1",
   "metadata": {},
   "source": [
    "#8\n",
    "\n",
    "What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa4fb9-5187-4fbd-ae60-900b0518145f",
   "metadata": {},
   "source": [
    "---\n",
    "The main advantage of polynomial regression over linear regression is its ability to model more complex, nonlinear relationships between the dependent and independent variables. Linear regression assumes a linear relationship between the dependent and independent variables, while polynomial regression can model more complex, nonlinear relationships.\n",
    "\n",
    "However, there are also some disadvantages to using polynomial regression. It can be more complex than linear regression, as it involves fitting a polynomial function rather than a straight line. Additionally, polynomial regression can be more prone to overfitting, especially if the degree of the polynomial is high.\n",
    "There are several ways to determine if I would  use polynomial regression compared to a simpler model like linear regression. One way is to create a scatterplot of the predictor variable and the response variable. If the relationship looks nonlinear, then it may be a good idea to fit a polynomial regression model instead.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91756b-fc97-4478-83a0-6bb261278238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
